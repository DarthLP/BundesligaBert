





# Install core dependencies
get_ipython().getoutput("pip install -q transformers datasets accelerate scikit-learn")

# Note: torch, pandas, numpy are pre-installed in Kaggle environment
# We'll install plotting libraries later after basic training works






# Hyperparameter configuration
CONFIG = {
    'model_name': 'distilbert-base-german-cased',
    'learning_rate': 1e-5,
    'num_train_epochs': 15,
    'per_device_train_batch_size': 16,
    'per_device_eval_batch_size': 32,
    'weight_decay': 0.01,
    'max_length': 512,
    'early_stopping_patience': 10,
    'random_seed': 42
}

# Dataset path (update with your dataset name)
DATASET_NAME = 'bundesligabert'  # Change this to your actual dataset name
DATASET_PATH = f'/kaggle/input/{DATASET_NAME}'

print("Configuration:")
for key, value in CONFIG.items():
    print(f"  {key}: {value}")
print(f"\nDataset path: {DATASET_PATH}")






import json
import os
from datasets import load_dataset

# List available files
print("Available files in dataset:")
for dirname, _, filenames in os.walk(DATASET_PATH):
    for filename in filenames:
        print(f"  {os.path.join(dirname, filename)}")

# Load JSON files
data_files = {
    'train': f'{DATASET_PATH}/train.json',
    'val': f'{DATASET_PATH}/val.json',
    'test_history': f'{DATASET_PATH}/test_history.json',
    'test_future': f'{DATASET_PATH}/test_future.json'
}

# Load datasets
datasets = load_dataset('json', data_files=data_files)

print("\nDatasets loaded successfully!")
print(f"Train: {len(datasets['train'])} samples")
print(f"Val: {len(datasets['val'])} samples")
print(f"Test History: {len(datasets['test_history'])} samples")
print(f"Test Future: {len(datasets['test_future'])} samples")









print(datasets.column_names)
# {'train': [...], 'val': [...], 'test_history': [...], 'test_future': [...]}

print(datasets["train"][0])          # first row of train
print(datasets["val"][0])            # first row of val
print(datasets["test_history"][0])   # etc.



import math

def log_transform_labels(example):
    example["label"] = math.log1p(example["label"])
    return example

print("Applying log(1 + y) label transform...")
datasets = datasets.map(log_transform_labels)


from transformers import AutoTokenizer

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(
    tokenizer=tokenizer,
    pad_to_multiple_of=8  # important for Tensor Cores on T4
)

# Tokenization function
def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        truncation=True,
        padding='max_length',
        max_length=CONFIG['max_length']
    )

# Tokenize all datasets
print("Tokenizing datasets...")
train_dataset = datasets['train'].map(tokenize_function, batched=True)
val_dataset = datasets['val'].map(tokenize_function, batched=True)
test_history_dataset = datasets['test_history'].map(tokenize_function, batched=True)
test_future_dataset = datasets['test_future'].map(tokenize_function, batched=True)

# Set format for PyTorch
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_history_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_future_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

print("Tokenization complete!")












import torch
from transformers import AutoModelForSequenceClassification
import numpy as np

# Set random seeds for reproducibility
torch.manual_seed(CONFIG['random_seed'])
np.random.seed(CONFIG['random_seed'])

# Check device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDA Version: {torch.version.cuda}")

# Initialize model
print(f"\nLoading model: {CONFIG['model_name']}")
model = AutoModelForSequenceClassification.from_pretrained(
    CONFIG['model_name'],
    num_labels=1  # Regression task
)

# Move model to device
model = model.to(device)
print("Model initialized and moved to device!")









print(train_dataset[0])



from transformers import Trainer, TrainingArguments, EarlyStoppingCallback
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Compute metrics function
def compute_metrics(eval_pred):
    preds, labels = eval_pred

    preds = preds.squeeze()
    labels = labels.squeeze()

    # back-transform from log(1 + y)
    preds = np.expm1(preds)
    labels = np.expm1(labels)

    rmse = np.sqrt(np.mean((preds - labels) ** 2))
    mae = np.mean(np.abs(preds - labels))

    return {
        "rmse": rmse,
        "mae": mae
    }
    
class RegressionTrainer(Trainer):
    # Added num_items_in_batch=None to the signature to fix the TypeError
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        preds = outputs.logits.squeeze(-1)

        loss_fn = torch.nn.SmoothL1Loss()
        loss = loss_fn(preds, labels)

        return (loss, outputs) if return_outputs else loss
        
# Training arguments (Unchanged)
training_args = TrainingArguments(
    output_dir="/kaggle/working/checkpoints",
    num_train_epochs=15,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=1e-5,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="rmse",
    greater_is_better=False,
    logging_steps=50,
    save_total_limit=3,
    fp16=True,
    report_to="none"
)

# --- FIX: Updated Trainer Initialization ---
trainer = RegressionTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    processing_class=tokenizer,  # Changed from 'tokenizer' to 'processing_class' to fix warning
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]
)

batch = next(iter(trainer.get_train_dataloader()))
print(batch["input_ids"].shape)



# Train
print("Starting training...")
train_result = trainer.train()
print(f"Training completed! Final loss: {train_result.training_loss:.4f}")








import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import json
import os

# 1. Setup Environment
# ------------------------------------------------------------------------------
output_dir = '/kaggle/working/figures'
os.makedirs(output_dir, exist_ok=True)
sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300

print("ðŸš€ Starting Post-Training Pipeline (Robust Version)...")

# 2. Prediction Function (Fixed for Lazy Evaluation)
# ------------------------------------------------------------------------------
def get_predictions(trainer, dataset, split_name):
    """Get predictions, extract metadata safely, and convert back to minutes."""
    if len(dataset) == 0:
        return {'predictions': [], 'labels': [], 'match_ids': [], 'seasons': [], 'halves': []}

    # A. Extract Metadata (Safely)
    # We switch to None to see all columns
    dataset.set_format(type=None)
    
    # --- FIX IS HERE: Force conversion to list to materialize data immediately ---
    # This prevents the "KeyError" when the format switches back to torch later
    match_ids = list(dataset['match_id'])
    seasons = list(dataset['season'])
    halves = list(dataset['half'])
    
    # B. Generate Predictions
    # Switch back to torch so the model can run
    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
    
    print(f"  > Predicting {split_name}...")
    pred_output = trainer.predict(dataset)
    raw_preds = pred_output.predictions.flatten()
    raw_labels = pred_output.label_ids.flatten()
    
    # C. Post-Process (Log -> Minutes)
    # Inverse log(1+x) using expm1
    real_preds = np.expm1(raw_preds)
    real_labels = np.expm1(raw_labels)
    
    # Clip negative predictions
    real_preds = np.maximum(0, real_preds)

    return {
        'predictions': real_preds,
        'labels': real_labels,
        'match_ids': match_ids,  # Now a safe list
        'seasons': seasons,      # Now a safe list
        'halves': halves         # Now a safe list
    }

# Generate Predictions
train_data = get_predictions(trainer, train_dataset, 'train')
val_data = get_predictions(trainer, val_dataset, 'val')
hist_data = get_predictions(trainer, test_history_dataset, 'test_history')
fut_data = get_predictions(trainer, test_future_dataset, 'test_future')

# 3. Dataframe Creation & Metrics
# ------------------------------------------------------------------------------
def create_df(data_dict):
    if len(data_dict['predictions']) == 0: return pd.DataFrame()
    df = pd.DataFrame({
        'match_id': data_dict['match_ids'],
        'season': data_dict['seasons'],
        'half': data_dict['halves'],
        'actual': data_dict['labels'],
        'predicted': data_dict['predictions']
    })
    df['residual'] = df['actual'] - df['predicted']
    df['abs_error'] = np.abs(df['residual'])
    return df

train_df = create_df(train_data)
val_df = create_df(val_data)
hist_df = create_df(hist_data)
fut_df = create_df(fut_data)

# Calculate Metrics Dictionary
metrics_summary = {}
for name, df in [('Train', train_df), ('Val', val_df), ('History', hist_df), ('Future', fut_df)]:
    if not df.empty:
        metrics_summary[name] = {
            'RMSE': float(np.sqrt(mean_squared_error(df['actual'], df['predicted']))),
            'MAE': float(mean_absolute_error(df['actual'], df['predicted'])),
            'R2': float(r2_score(df['actual'], df['predicted']))
        }
    else:
        metrics_summary[name] = "No Data"

print("\nðŸ“Š METRICS SUMMARY (Real Minutes):")
print(json.dumps(metrics_summary, indent=2))

# 4. Diagnostic Plotting (All 10 Plots)
# ------------------------------------------------------------------------------
print("\nðŸŽ¨ Generating 10 Diagnostic Plots...")

# Plot 1: Learning Curve
history = trainer.state.log_history
train_loss = [x['loss'] for x in history if 'loss' in x]
eval_loss = [x['eval_loss'] for x in history if 'eval_loss' in x]
if train_loss:
    plt.figure(figsize=(10, 6))
    plt.plot(train_loss, label='Training Loss')
    if eval_loss:
        steps = np.linspace(0, len(train_loss), len(eval_loss))
        plt.plot(steps, eval_loss, 'o-', label='Validation Loss')
    plt.title('1. Learning Curve (Log-Loss)')
    plt.xlabel('Steps'); plt.ylabel('Loss')
    plt.legend(); plt.grid(True, alpha=0.3)
    plt.savefig(f'{output_dir}/1_learning_curve.png', bbox_inches='tight'); plt.close()

# Plot 2: REC Curve (CDF of Errors)
plt.figure(figsize=(10, 6))
tols = np.linspace(0, 5, 100)
for name, df in [('History', hist_df), ('Future', fut_df)]:
    if not df.empty:
        for half in [45, 90]:
            sub = df[df['half'] == half]
            if not sub.empty:
                rec = [100 * (sub['abs_error'] <= t).mean() for t in tols]
                plt.plot(tols, rec, label=f'{name} (Half {half})', lw=2)
plt.title('2. REC Curve (Accuracy vs Tolerance)'); plt.xlabel('Tolerance (Minutes)'); plt.ylabel('% Correct')
plt.legend(); plt.grid(True, alpha=0.3)
plt.savefig(f'{output_dir}/2_rec_curve.png', bbox_inches='tight'); plt.close()

# Plot 3: Scatter (Actual vs Predicted)
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
for ax, df, title in [(ax1, hist_df, 'History'), (ax2, fut_df, 'Future')]:
    if not df.empty:
        sns.scatterplot(data=df, x='actual', y='predicted', hue='half', style='half', alpha=0.6, ax=ax)
        mx = max(df.actual.max(), df.predicted.max()) + 1
        ax.plot([0, mx], [0, mx], 'k--', lw=1.5, label='Perfect Fit')
        
        r2_val = metrics_summary.get(title, {}).get('R2', 0) if isinstance(metrics_summary.get(title), dict) else 0
        ax.set_title(f"{title} (RÂ²={r2_val:.2f})")
        ax.legend()
    else: ax.text(0.5, 0.5, 'No Data', transform=ax.transAxes)
plt.savefig(f'{output_dir}/3_scatter.png', bbox_inches='tight'); plt.close()

# Plot 4: Residual Distribution
plt.figure(figsize=(10, 6))
if not hist_df.empty: sns.histplot(hist_df['residual'], kde=True, bins=30, label='History', color='blue', alpha=0.4)
if not fut_df.empty: sns.histplot(fut_df['residual'], kde=True, bins=30, label='Future', color='orange', alpha=0.4)
plt.axvline(0, color='k', linestyle='--')
plt.title('4. Residual Distribution'); plt.xlabel('Residual (Actual - Predicted)'); plt.legend()
plt.savefig(f'{output_dir}/4_residual_dist.png', bbox_inches='tight'); plt.close()

# Plot 5: Q-Q Plot
plt.figure(figsize=(8, 8))
if not hist_df.empty:
    stats.probplot(hist_df['residual'], dist="norm", plot=plt)
    plt.title('5. Q-Q Plot (History Residuals)')
plt.savefig(f'{output_dir}/5_qq_plot.png', bbox_inches='tight'); plt.close()

# Plot 6: Residuals vs Predicted
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
for ax, df, title in [(ax1, hist_df, 'History'), (ax2, fut_df, 'Future')]:
    if not df.empty:
        ax.scatter(df['predicted'], df['residual'], alpha=0.5)
        ax.axhline(0, color='k', linestyle='--')
        ax.set_title(f"6. {title}: Residuals vs Predicted")
        ax.set_xlabel('Predicted Minutes'); ax.set_ylabel('Residual')
plt.savefig(f'{output_dir}/6_res_vs_pred.png', bbox_inches='tight'); plt.close()

# Plot 7: Error by Season
combined = pd.concat([hist_df.assign(split='History'), fut_df.assign(split='Future')]) if not hist_df.empty or not fut_df.empty else pd.DataFrame()
if not combined.empty and 'season' in combined.columns:
    plt.figure(figsize=(12, 6))
    sns.boxplot(data=combined, x='season', y='abs_error', hue='split')
    plt.xticks(rotation=45)
    plt.title('7. Absolute Error by Season'); plt.ylabel('Abs Error (Min)')
    plt.savefig(f'{output_dir}/7_error_by_season.png', bbox_inches='tight'); plt.close()

# Plot 8: Error by Half
if not combined.empty:
    plt.figure(figsize=(8, 6))
    sns.boxplot(data=combined, x='half', y='abs_error', hue='split')
    plt.title('8. Absolute Error by Half'); plt.ylabel('Abs Error (Min)')
    plt.savefig(f'{output_dir}/8_error_by_half.png', bbox_inches='tight'); plt.close()

# Plot 9: Calibration Plot
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
for ax, df, title in [(ax1, hist_df, 'History'), (ax2, fut_df, 'Future')]:
    if not df.empty:
        if df.predicted.nunique() > 1:
            bins = np.linspace(df.predicted.min(), df.predicted.max(), 10)
            grouped = df.groupby(pd.cut(df.predicted, bins, include_lowest=True))
            bin_means = grouped['actual'].mean()
            bin_centers = grouped['predicted'].mean()
            ax.scatter(bin_centers, bin_means, s=100, zorder=5)
        else:
            ax.scatter(df.predicted.mean(), df.actual.mean(), s=100)
        ax.plot([0, 15], [0, 15], 'r--', label='Perfect Calibration')
        ax.set_title(f"9. {title} Calibration")
        ax.set_xlabel('Mean Predicted'); ax.set_ylabel('Mean Actual')
        ax.legend()
plt.savefig(f'{output_dir}/9_calibration.png', bbox_inches='tight'); plt.close()

# Plot 10: Error Distribution Comparison (Violin)
if not combined.empty:
    plt.figure(figsize=(10, 6))
    sns.violinplot(data=combined, x='half', y='abs_error', hue='split', split=True, inner='quart')
    plt.title('10. Error Distribution Density by Half')
    plt.savefig(f'{output_dir}/10_violin_dist.png', bbox_inches='tight'); plt.close()

print("âœ… All plots saved to /kaggle/working/figures/")

# 5. Save Files
# ------------------------------------------------------------------------------
print("\nðŸ’¾ Saving CSVs and JSONs...")
if not train_df.empty: train_df.to_csv('/kaggle/working/bert_predictions_train.csv', index=False)
if not val_df.empty: val_df.to_csv('/kaggle/working/bert_predictions_val.csv', index=False)
if not hist_df.empty: hist_df.to_csv('/kaggle/working/bert_predictions_history.csv', index=False)
if not fut_df.empty: fut_df.to_csv('/kaggle/working/bert_predictions_future.csv', index=False)

with open('/kaggle/working/bert_performance.json', 'w') as f:
    json.dump(metrics_summary, f, indent=2)

print("âœ… DONE! All outputs ready.")
