{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BundesligaBERT: BERT Fine-Tuning on Kaggle GPU\n",
        "\n",
        "This notebook fine-tunes a German BERT model (`distilbert-base-german-cased`) to predict Bundesliga stoppage time from live ticker text.\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. **Upload Data**: Upload the JSON files (train.json, val.json, test_history.json, test_future.json) as a Kaggle dataset\n",
        "2. **Attach Dataset**: Click \"Add data\" → Search for your dataset → Add it\n",
        "3. **Enable GPU**: Settings → Accelerator → GPU T4 x2 (or available GPU)\n",
        "4. **Run All**: Run all cells to train the model and generate diagnostics\n",
        "\n",
        "## Outputs\n",
        "\n",
        "All outputs are saved to `/kaggle/working/`:\n",
        "- Predictions CSV files for all splits\n",
        "- Performance metrics JSON\n",
        "- Training log JSON\n",
        "- Aggregated statistics JSON\n",
        "- 10 diagnostic plots (PNG)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup: Install Core Dependencies\n",
        "\n",
        "Install core libraries needed for training. Plotting libraries will be installed later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install core dependencies\n",
        "!pip install -q transformers datasets accelerate scikit-learn\n",
        "\n",
        "# Note: torch, pandas, numpy are pre-installed in Kaggle environment\n",
        "# We'll install plotting libraries later after basic training works\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Hyperparameters for easy tweaking without editing code below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter configuration\n",
        "CONFIG = {\n",
        "    'model_name': 'distilbert-base-german-cased',\n",
        "    'learning_rate': 2e-5,\n",
        "    'num_train_epochs': 10,\n",
        "    'per_device_train_batch_size': 16,\n",
        "    'per_device_eval_batch_size': 32,\n",
        "    'weight_decay': 0.01,\n",
        "    'max_length': 512,\n",
        "    'early_stopping_patience': 3,\n",
        "    'random_seed': 42\n",
        "}\n",
        "\n",
        "# Dataset path (update with your dataset name)\n",
        "DATASET_NAME = 'your-dataset-name'  # Change this to your actual dataset name\n",
        "DATASET_PATH = f'/kaggle/input/{DATASET_NAME}'\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(f\"\\nDataset path: {DATASET_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading\n",
        "\n",
        "Load JSON files from the Kaggle dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "\n",
        "# List available files\n",
        "print(\"Available files in dataset:\")\n",
        "for dirname, _, filenames in os.walk(DATASET_PATH):\n",
        "    for filename in filenames:\n",
        "        print(f\"  {os.path.join(dirname, filename)}\")\n",
        "\n",
        "# Load JSON files\n",
        "data_files = {\n",
        "    'train': f'{DATASET_PATH}/train.json',\n",
        "    'val': f'{DATASET_PATH}/val.json',\n",
        "    'test_history': f'{DATASET_PATH}/test_history.json',\n",
        "    'test_future': f'{DATASET_PATH}/test_future.json'\n",
        "}\n",
        "\n",
        "# Load datasets\n",
        "datasets = load_dataset('json', data_files=data_files)\n",
        "\n",
        "print(\"\\nDatasets loaded successfully!\")\n",
        "print(f\"Train: {len(datasets['train'])} samples\")\n",
        "print(f\"Val: {len(datasets['val'])} samples\")\n",
        "print(f\"Test History: {len(datasets['test_history'])} samples\")\n",
        "print(f\"Test Future: {len(datasets['test_future'])} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Preparation\n",
        "\n",
        "Tokenize the text data using the BERT tokenizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=CONFIG['max_length']\n",
        "    )\n",
        "\n",
        "# Tokenize all datasets\n",
        "print(\"Tokenizing datasets...\")\n",
        "train_dataset = datasets['train'].map(tokenize_function, batched=True)\n",
        "val_dataset = datasets['val'].map(tokenize_function, batched=True)\n",
        "test_history_dataset = datasets['test_history'].map(tokenize_function, batched=True)\n",
        "test_future_dataset = datasets['test_future'].map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_history_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_future_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "print(\"Tokenization complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Validation\n",
        "\n",
        "Validate dataset sizes and distributions to catch issues before training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def validate_dataset(dataset, name):\n",
        "    \"\"\"Validate and print dataset statistics.\"\"\"\n",
        "    if len(dataset) == 0:\n",
        "        print(f\"⚠️  WARNING: {name} is EMPTY!\")\n",
        "        return\n",
        "    \n",
        "    # Convert to pandas for easier analysis\n",
        "    df = pd.DataFrame({\n",
        "        'match_id': dataset['match_id'],\n",
        "        'season': dataset['season'],\n",
        "        'half': dataset['half'],\n",
        "        'label': dataset['label']\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Total samples: {len(df)}\")\n",
        "    print(f\"  Half distribution:\")\n",
        "    print(f\"    45: {(df['half'] == 45).sum()} ({(df['half'] == 45).sum() / len(df) * 100:.1f}%)\")\n",
        "    print(f\"    90: {(df['half'] == 90).sum()} ({(df['half'] == 90).sum() / len(df) * 100:.1f}%)\")\n",
        "    print(f\"  Season distribution:\")\n",
        "    for season in sorted(df['season'].unique()):\n",
        "        count = (df['season'] == season).sum()\n",
        "        print(f\"    {season}: {count} ({(count / len(df) * 100):.1f}%)\")\n",
        "    print(f\"  Label statistics:\")\n",
        "    print(f\"    Mean: {df['label'].mean():.2f}\")\n",
        "    print(f\"    Std: {df['label'].std():.2f}\")\n",
        "    print(f\"    Min: {df['label'].min():.2f}\")\n",
        "    print(f\"    Max: {df['label'].max():.2f}\")\n",
        "    \n",
        "    if len(df) < 10:\n",
        "        print(f\"  ⚠️  WARNING: {name} has very few samples ({len(df)})\")\n",
        "\n",
        "# Validate all datasets\n",
        "validate_dataset(train_dataset, \"Train\")\n",
        "validate_dataset(val_dataset, \"Validation\")\n",
        "validate_dataset(test_history_dataset, \"Test History\")\n",
        "validate_dataset(test_future_dataset, \"Test Future\")\n",
        "\n",
        "print(\"\\n✅ Data validation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Initialization\n",
        "\n",
        "Initialize the BERT model for sequence classification (regression).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(CONFIG['random_seed'])\n",
        "np.random.seed(CONFIG['random_seed'])\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "\n",
        "# Initialize model\n",
        "print(f\"\\nLoading model: {CONFIG['model_name']}\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    CONFIG['model_name'],\n",
        "    num_labels=1  # Regression task\n",
        ")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "print(\"Model initialized and moved to device!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training\n",
        "\n",
        "Configure and train the BERT model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Compute metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions.flatten()\n",
        "    labels = labels.flatten()\n",
        "    \n",
        "    rmse = np.sqrt(mean_squared_error(labels, predictions))\n",
        "    mae = mean_absolute_error(labels, predictions)\n",
        "    r2 = r2_score(labels, predictions)\n",
        "    \n",
        "    return {\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'r2': r2\n",
        "    }\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/kaggle/working/checkpoints',\n",
        "    num_train_epochs=CONFIG['num_train_epochs'],\n",
        "    per_device_train_batch_size=CONFIG['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=CONFIG['per_device_eval_batch_size'],\n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    weight_decay=CONFIG['weight_decay'],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    greater_is_better=False,\n",
        "    logging_steps=50,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    save_total_limit=3,\n",
        "    report_to='none'  # Disable wandb/tensorboard\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=CONFIG['early_stopping_patience'])]\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "print(f\"Training completed! Final loss: {train_result.training_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Predictions & Metrics\n",
        "\n",
        "Generate predictions for all splits and extract metrics. `trainer.predict()` returns both metrics and predictions in one call.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to get predictions and metadata\n",
        "def get_predictions(trainer, dataset, split_name):\n",
        "    \"\"\"Get predictions and extract metadata.\"\"\"\n",
        "    # Temporarily change format to access metadata\n",
        "    dataset.set_format(type=None)\n",
        "    \n",
        "    # Extract metadata\n",
        "    metadata = {\n",
        "        'match_ids': dataset['match_id'],\n",
        "        'seasons': dataset['season'],\n",
        "        'halves': dataset['half']\n",
        "    }\n",
        "    \n",
        "    # Reset format for prediction\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    \n",
        "    # Get predictions (returns both metrics and predictions)\n",
        "    pred_output = trainer.predict(dataset)\n",
        "    \n",
        "    return {\n",
        "        'predictions': pred_output.predictions.flatten(),\n",
        "        'labels': pred_output.label_ids.flatten(),\n",
        "        'metrics': pred_output.metrics,\n",
        "        'match_ids': metadata['match_ids'],\n",
        "        'seasons': metadata['seasons'],\n",
        "        'halves': metadata['halves']\n",
        "    }\n",
        "\n",
        "# Get predictions for all splits\n",
        "print(\"Generating predictions...\")\n",
        "train_predictions = get_predictions(trainer, train_dataset, 'train')\n",
        "val_predictions = get_predictions(trainer, val_dataset, 'val')\n",
        "test_history_predictions = get_predictions(trainer, test_history_dataset, 'test_history')\n",
        "test_future_predictions = get_predictions(trainer, test_future_dataset, 'test_future')\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"METRICS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "for name, preds in [\n",
        "    ('Train', train_predictions),\n",
        "    ('Validation', val_predictions),\n",
        "    ('Test History', test_history_predictions),\n",
        "    ('Test Future', test_future_predictions)\n",
        "]:\n",
        "    if len(preds['predictions']) > 0:\n",
        "        metrics = preds['metrics']\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"  RMSE: {metrics.get('test_rmse', 'N/A'):.4f}\")\n",
        "        print(f\"  MAE: {metrics.get('test_mae', 'N/A'):.4f}\")\n",
        "        print(f\"  R²: {metrics.get('test_r2', 'N/A'):.4f}\")\n",
        "    else:\n",
        "        print(f\"\\n{name}: No data\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Statistics\n",
        "\n",
        "Calculate aggregated statistics (mean, std, min, max) per split/subset for local comparison with regression results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_subset_metrics(predictions, labels, halves):\n",
        "    \"\"\"Calculate metrics for combined, subset_45, and subset_90.\"\"\"\n",
        "    predictions = np.array(predictions)\n",
        "    labels = np.array(labels)\n",
        "    halves = np.array(halves)\n",
        "    \n",
        "    if len(predictions) == 0 or len(labels) == 0:\n",
        "        return {'combined': {}, 'subset_45': {}, 'subset_90': {}}\n",
        "    \n",
        "    combined = {\n",
        "        'rmse': float(np.sqrt(mean_squared_error(labels, predictions))),\n",
        "        'mae': float(mean_absolute_error(labels, predictions)),\n",
        "        'r2': float(r2_score(labels, predictions)),\n",
        "        'mean_actual': float(np.mean(labels)),\n",
        "        'mean_predicted': float(np.mean(predictions)),\n",
        "        'std_actual': float(np.std(labels)),\n",
        "        'std_predicted': float(np.std(predictions)),\n",
        "        'min_actual': float(np.min(labels)),\n",
        "        'max_actual': float(np.max(labels)),\n",
        "        'min_predicted': float(np.min(predictions)),\n",
        "        'max_predicted': float(np.max(predictions))\n",
        "    }\n",
        "    \n",
        "    subset_45_mask = halves == 45\n",
        "    subset_90_mask = halves == 90\n",
        "    \n",
        "    subset_45 = {}\n",
        "    subset_90 = {}\n",
        "    \n",
        "    if subset_45_mask.sum() > 0:\n",
        "        subset_45 = {\n",
        "            'rmse': float(np.sqrt(mean_squared_error(labels[subset_45_mask], predictions[subset_45_mask]))),\n",
        "            'mae': float(mean_absolute_error(labels[subset_45_mask], predictions[subset_45_mask])),\n",
        "            'r2': float(r2_score(labels[subset_45_mask], predictions[subset_45_mask])),\n",
        "            'mean_actual': float(np.mean(labels[subset_45_mask])),\n",
        "            'mean_predicted': float(np.mean(predictions[subset_45_mask])),\n",
        "            'std_actual': float(np.std(labels[subset_45_mask])),\n",
        "            'std_predicted': float(np.std(predictions[subset_45_mask])),\n",
        "            'min_actual': float(np.min(labels[subset_45_mask])),\n",
        "            'max_actual': float(np.max(labels[subset_45_mask])),\n",
        "            'min_predicted': float(np.min(predictions[subset_45_mask])),\n",
        "            'max_predicted': float(np.max(predictions[subset_45_mask]))\n",
        "        }\n",
        "    \n",
        "    if subset_90_mask.sum() > 0:\n",
        "        subset_90 = {\n",
        "            'rmse': float(np.sqrt(mean_squared_error(labels[subset_90_mask], predictions[subset_90_mask]))),\n",
        "            'mae': float(mean_absolute_error(labels[subset_90_mask], predictions[subset_90_mask])),\n",
        "            'r2': float(r2_score(labels[subset_90_mask], predictions[subset_90_mask])),\n",
        "            'mean_actual': float(np.mean(labels[subset_90_mask])),\n",
        "            'mean_predicted': float(np.mean(predictions[subset_90_mask])),\n",
        "            'std_actual': float(np.std(labels[subset_90_mask])),\n",
        "            'std_predicted': float(np.std(predictions[subset_90_mask])),\n",
        "            'min_actual': float(np.min(labels[subset_90_mask])),\n",
        "            'max_actual': float(np.max(labels[subset_90_mask])),\n",
        "            'min_predicted': float(np.min(predictions[subset_90_mask])),\n",
        "            'max_predicted': float(np.max(predictions[subset_90_mask]))\n",
        "        }\n",
        "    \n",
        "    return {'combined': combined, 'subset_45': subset_45, 'subset_90': subset_90}\n",
        "\n",
        "# Calculate statistics for all splits\n",
        "performance_metrics = {\n",
        "    'train': calculate_subset_metrics(\n",
        "        train_predictions['predictions'],\n",
        "        train_predictions['labels'],\n",
        "        train_predictions['halves']\n",
        "    ),\n",
        "    'val': calculate_subset_metrics(\n",
        "        val_predictions['predictions'],\n",
        "        val_predictions['labels'],\n",
        "        val_predictions['halves']\n",
        "    ),\n",
        "    'test_history': calculate_subset_metrics(\n",
        "        test_history_predictions['predictions'],\n",
        "        test_history_predictions['labels'],\n",
        "        test_history_predictions['halves']\n",
        "    ),\n",
        "    'test_future': calculate_subset_metrics(\n",
        "        test_future_predictions['predictions'],\n",
        "        test_future_predictions['labels'],\n",
        "        test_future_predictions['halves']\n",
        "    )\n",
        "}\n",
        "\n",
        "print(\"Statistics calculated for all splits!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install plotting libraries\n",
        "!pip install -q matplotlib seaborn scipy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Diagnostic Plots\n",
        "\n",
        "Generate all 10 diagnostic plots for model evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "\n",
        "# Create output directory\n",
        "output_dir = '/kaggle/working/figures'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Prepare DataFrames\n",
        "def prepare_df(predictions_dict, name):\n",
        "    \"\"\"Prepare DataFrame from predictions dictionary.\"\"\"\n",
        "    if len(predictions_dict['predictions']) == 0:\n",
        "        return pd.DataFrame()\n",
        "    return pd.DataFrame({\n",
        "        'match_id': predictions_dict['match_ids'],\n",
        "        'season': predictions_dict['seasons'],\n",
        "        'half': predictions_dict['halves'],\n",
        "        'actual': predictions_dict['labels'],\n",
        "        'predicted': predictions_dict['predictions']\n",
        "    })\n",
        "\n",
        "history_df = prepare_df(test_history_predictions, 'history')\n",
        "future_df = prepare_df(test_future_predictions, 'future')\n",
        "\n",
        "if len(history_df) > 0:\n",
        "    history_df['residual'] = history_df['actual'] - history_df['predicted']\n",
        "    history_df['abs_error'] = np.abs(history_df['residual'])\n",
        "\n",
        "if len(future_df) > 0:\n",
        "    future_df['residual'] = future_df['actual'] - future_df['predicted']\n",
        "    future_df['abs_error'] = np.abs(future_df['residual'])\n",
        "\n",
        "print(\"DataFrames prepared for plotting\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Learning Curve\n",
        "print(\"Plotting learning curve...\")\n",
        "history = trainer.state.log_history\n",
        "train_losses = [h['loss'] for h in history if 'loss' in h]\n",
        "eval_losses = [h['eval_loss'] for h in history if 'eval_loss' in h]\n",
        "eval_steps = [h['step'] for h in history if 'eval_loss' in h]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "if train_losses:\n",
        "    steps = list(range(len(train_losses)))\n",
        "    ax.plot(steps, train_losses, label='Training Loss', alpha=0.7)\n",
        "if eval_losses:\n",
        "    ax.plot(eval_steps, eval_losses, label='Validation Loss', marker='o', markersize=4)\n",
        "ax.set_xlabel('Step')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Learning Curve: Training vs Validation Loss')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{output_dir}/learning_curve.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"✓ Learning curve saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. CDF of Absolute Errors (Rec Curve)\n",
        "print(\"Plotting CDF of absolute errors...\")\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "tolerances = np.linspace(0, 5, 100)\n",
        "\n",
        "for name, df, half in [\n",
        "    ('History-45', history_df[history_df['half'] == 45] if len(history_df) > 0 else pd.DataFrame(), 45),\n",
        "    ('History-90', history_df[history_df['half'] == 90] if len(history_df) > 0 else pd.DataFrame(), 90),\n",
        "    ('Future-45', future_df[future_df['half'] == 45] if len(future_df) > 0 else pd.DataFrame(), 45),\n",
        "    ('Future-90', future_df[future_df['half'] == 90] if len(future_df) > 0 else pd.DataFrame(), 90)\n",
        "]:\n",
        "    if len(df) > 0:\n",
        "        percentages = [100 * (df['abs_error'] <= tol).sum() / len(df) for tol in tolerances]\n",
        "        ax.plot(tolerances, percentages, label=name, linewidth=2)\n",
        "\n",
        "ax.set_xlabel('Tolerance (minutes)')\n",
        "ax.set_ylabel('% of Predictions within Tolerance')\n",
        "ax.set_title('CDF of Absolute Errors (Rec Curve)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{output_dir}/rec_curve.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"✓ Rec curve saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Prediction vs Actual Scatter\n",
        "print(\"Plotting prediction vs actual scatter...\")\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# History\n",
        "if len(history_df) > 0:\n",
        "    for half in [45, 90]:\n",
        "        subset = history_df[history_df['half'] == half]\n",
        "        if len(subset) > 0:\n",
        "            color = 'blue' if half == 45 else 'red'\n",
        "            ax1.scatter(subset['actual'], subset['predicted'], alpha=0.5, label=f'Half {half}', color=color, s=20)\n",
        "    \n",
        "    max_val = max(history_df['actual'].max(), history_df['predicted'].max())\n",
        "    ax1.plot([0, max_val], [0, max_val], 'k--', label='Perfect Fit', linewidth=2)\n",
        "    ax1.set_xlabel('Actual')\n",
        "    ax1.set_ylabel('Predicted')\n",
        "    try:\n",
        "        r2_hist = r2_score(history_df[\"actual\"], history_df[\"predicted\"])\n",
        "        ax1.set_title(f'History (R² = {r2_hist:.3f})')\n",
        "    except:\n",
        "        ax1.set_title('History')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "else:\n",
        "    ax1.text(0.5, 0.5, 'No History data', ha='center', va='center', transform=ax1.transAxes)\n",
        "    ax1.set_title('History')\n",
        "\n",
        "# Future\n",
        "if len(future_df) > 0:\n",
        "    for half in [45, 90]:\n",
        "        subset = future_df[future_df['half'] == half]\n",
        "        if len(subset) > 0:\n",
        "            color = 'blue' if half == 45 else 'red'\n",
        "            ax2.scatter(subset['actual'], subset['predicted'], alpha=0.5, label=f'Half {half}', color=color, s=20)\n",
        "    \n",
        "    max_val = max(future_df['actual'].max(), future_df['predicted'].max())\n",
        "    ax2.plot([0, max_val], [0, max_val], 'k--', label='Perfect Fit', linewidth=2)\n",
        "    ax2.set_xlabel('Actual')\n",
        "    ax2.set_ylabel('Predicted')\n",
        "    try:\n",
        "        r2_fut = r2_score(future_df[\"actual\"], future_df[\"predicted\"])\n",
        "        ax2.set_title(f'Future (R² = {r2_fut:.3f})')\n",
        "    except:\n",
        "        ax2.set_title('Future')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "else:\n",
        "    ax2.text(0.5, 0.5, 'No Future data', ha='center', va='center', transform=ax2.transAxes)\n",
        "    ax2.set_title('Future')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{output_dir}/pred_vs_actual_scatter.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"✓ Prediction vs actual scatter saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Residual Distribution\n",
        "print(\"Plotting residual distribution...\")\n",
        "if len(history_df) > 0 or len(future_df) > 0:\n",
        "    all_residuals = pd.concat([history_df['residual'], future_df['residual']]) if len(history_df) > 0 and len(future_df) > 0 else (history_df['residual'] if len(history_df) > 0 else future_df['residual'])\n",
        "    if len(all_residuals) > 0:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        ax.hist(all_residuals, bins=50, density=True, alpha=0.7, label='Residuals', color='steelblue')\n",
        "        \n",
        "        # Fit Gaussian\n",
        "        mu, sigma = stats.norm.fit(all_residuals)\n",
        "        x = np.linspace(all_residuals.min(), all_residuals.max(), 100)\n",
        "        ax.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label=f'Gaussian Fit (μ={mu:.2f}, σ={sigma:.2f})')\n",
        "        ax.axvline(0, color='black', linestyle='--', linewidth=2, label='Zero')\n",
        "        ax.set_xlabel('Residual (Actual - Predicted)')\n",
        "        ax.set_ylabel('Density')\n",
        "        ax.set_title('Residual Distribution')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{output_dir}/residual_distribution.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(\"✓ Residual distribution saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Q-Q Plot\n",
        "print(\"Plotting Q-Q plot...\")\n",
        "if len(history_df) > 0 or len(future_df) > 0:\n",
        "    all_residuals = pd.concat([history_df['residual'], future_df['residual']]) if len(history_df) > 0 and len(future_df) > 0 else (history_df['residual'] if len(history_df) > 0 else future_df['residual'])\n",
        "    if len(all_residuals) > 0:\n",
        "        fig, ax = plt.subplots(figsize=(8, 8))\n",
        "        stats.probplot(all_residuals, dist=\"norm\", plot=ax)\n",
        "        ax.set_title('Q-Q Plot: Residuals vs Normal Distribution')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{output_dir}/qq_plot_residuals.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(\"✓ Q-Q plot saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Residuals vs Predicted\n",
        "print(\"Plotting residuals vs predicted...\")\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "if len(history_df) > 0:\n",
        "    ax1.scatter(history_df['predicted'], history_df['residual'], alpha=0.5, s=20)\n",
        "ax1.axhline(0, color='black', linestyle='--', linewidth=2)\n",
        "ax1.set_xlabel('Predicted')\n",
        "ax1.set_ylabel('Residual')\n",
        "ax1.set_title('History: Residuals vs Predicted')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "if len(future_df) > 0:\n",
        "    ax2.scatter(future_df['predicted'], future_df['residual'], alpha=0.5, s=20)\n",
        "ax2.axhline(0, color='black', linestyle='--', linewidth=2)\n",
        "ax2.set_xlabel('Predicted')\n",
        "ax2.set_ylabel('Residual')\n",
        "ax2.set_title('Future: Residuals vs Predicted')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{output_dir}/residuals_vs_predicted.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"✓ Residuals vs predicted saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Error by Season\n",
        "print(\"Plotting error by season...\")\n",
        "if len(history_df) > 0 or len(future_df) > 0:\n",
        "    combined_df = pd.concat([\n",
        "        history_df.assign(split='History') if len(history_df) > 0 else pd.DataFrame(),\n",
        "        future_df.assign(split='Future') if len(future_df) > 0 else pd.DataFrame()\n",
        "    ])\n",
        "    \n",
        "    if len(combined_df) > 0:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "        \n",
        "        for idx, split in enumerate(['History', 'Future']):\n",
        "            subset = combined_df[combined_df['split'] == split]\n",
        "            if len(subset) > 0 and 'season' in subset.columns:\n",
        "                subset.boxplot(column='abs_error', by='season', ax=axes[idx], grid=False)\n",
        "                axes[idx].set_title(f'{split}: Absolute Error by Season')\n",
        "                axes[idx].set_xlabel('Season')\n",
        "                axes[idx].set_ylabel('Absolute Error (minutes)')\n",
        "                plt.setp(axes[idx].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "            else:\n",
        "                axes[idx].text(0.5, 0.5, f'No data for {split}', ha='center', va='center', transform=axes[idx].transAxes)\n",
        "                axes[idx].set_title(f'{split}: Absolute Error by Season')\n",
        "        \n",
        "        plt.suptitle('')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{output_dir}/error_by_season.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(\"✓ Error by season saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. Error by Half\n",
        "print(\"Plotting error by half...\")\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "for idx, (name, df) in enumerate([('History', history_df), ('Future', future_df)]):\n",
        "    if len(df) > 0:\n",
        "        df.boxplot(column='abs_error', by='half', ax=axes[idx], grid=False)\n",
        "        axes[idx].set_title(f'{name}: Absolute Error by Half')\n",
        "        axes[idx].set_xlabel('Half')\n",
        "        axes[idx].set_ylabel('Absolute Error (minutes)')\n",
        "    else:\n",
        "        axes[idx].text(0.5, 0.5, f'No data for {name}', ha='center', va='center', transform=axes[idx].transAxes)\n",
        "        axes[idx].set_title(f'{name}: Absolute Error by Half')\n",
        "\n",
        "plt.suptitle('')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{output_dir}/error_by_half.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"✓ Error by half saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. Calibration Plot\n",
        "print(\"Plotting calibration plot...\")\n",
        "if len(history_df) > 0 or len(future_df) > 0:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    for idx, (name, df) in enumerate([('History', history_df), ('Future', future_df)]):\n",
        "        ax = ax1 if idx == 0 else ax2\n",
        "        if len(df) > 0 and 'predicted' in df.columns and 'actual' in df.columns:\n",
        "            try:\n",
        "                # Bin predictions\n",
        "                if df['predicted'].min() == df['predicted'].max():\n",
        "                    bin_centers = [df['predicted'].mean()]\n",
        "                    bin_means = [df['actual'].mean()]\n",
        "                else:\n",
        "                    bins = np.linspace(df['predicted'].min(), df['predicted'].max(), 10)\n",
        "                    df_temp = df.copy()\n",
        "                    df_temp['pred_bin'] = pd.cut(df_temp['predicted'], bins=bins)\n",
        "                    \n",
        "                    bin_centers = []\n",
        "                    bin_means = []\n",
        "                    for bin_group in df_temp.groupby('pred_bin'):\n",
        "                        bin_centers.append(bin_group[1]['predicted'].mean())\n",
        "                        bin_means.append(bin_group[1]['actual'].mean())\n",
        "                \n",
        "                if bin_centers:\n",
        "                    ax.scatter(bin_centers, bin_means, s=100, alpha=0.7)\n",
        "                    max_val = max(max(bin_centers), max(bin_means)) if bin_centers else 10\n",
        "                    ax.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Perfect Calibration')\n",
        "                    ax.set_xlabel('Mean Predicted (binned)')\n",
        "                    ax.set_ylabel('Mean Actual')\n",
        "                    ax.set_title(f'{name}: Calibration Plot')\n",
        "                    ax.legend()\n",
        "                    ax.grid(True, alpha=0.3)\n",
        "                else:\n",
        "                    ax.text(0.5, 0.5, f'No data for {name}', ha='center', va='center', transform=ax.transAxes)\n",
        "                    ax.set_title(f'{name}: Calibration Plot')\n",
        "            except Exception as e:\n",
        "                print(f\"Error creating calibration plot for {name}: {e}\")\n",
        "                ax.text(0.5, 0.5, f'Error plotting {name}', ha='center', va='center', transform=ax.transAxes)\n",
        "                ax.set_title(f'{name}: Calibration Plot')\n",
        "        else:\n",
        "            ax.text(0.5, 0.5, f'No data for {name}', ha='center', va='center', transform=ax.transAxes)\n",
        "            ax.set_title(f'{name}: Calibration Plot')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{output_dir}/calibration_plot.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"✓ Calibration plot saved\")\n",
        "\n",
        "print(\"\\n✅ All diagnostic plots generated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Save Outputs\n",
        "\n",
        "Save all predictions, metrics, and aggregated statistics to `/kaggle/working/` for download.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save predictions for all splits\n",
        "def save_predictions(predictions_dict, filename):\n",
        "    \"\"\"Save predictions to CSV.\"\"\"\n",
        "    if len(predictions_dict['predictions']) > 0:\n",
        "        df = pd.DataFrame({\n",
        "            'match_id': predictions_dict['match_ids'],\n",
        "            'season': predictions_dict['seasons'],\n",
        "            'half': predictions_dict['halves'],\n",
        "            'actual': predictions_dict['labels'],\n",
        "            'predicted': predictions_dict['predictions'],\n",
        "            'residual': predictions_dict['labels'] - predictions_dict['predictions'],\n",
        "            'abs_error': np.abs(predictions_dict['labels'] - predictions_dict['predictions'])\n",
        "        })\n",
        "        df.to_csv(f'/kaggle/working/{filename}', index=False)\n",
        "        print(f\"✓ Saved {filename} ({len(df)} samples)\")\n",
        "    else:\n",
        "        # Create empty DataFrame with correct columns\n",
        "        df = pd.DataFrame(columns=['match_id', 'season', 'half', 'actual', 'predicted', 'residual', 'abs_error'])\n",
        "        df.to_csv(f'/kaggle/working/{filename}', index=False)\n",
        "        print(f\"⚠️  {filename} is empty (no data)\")\n",
        "\n",
        "print(\"Saving predictions...\")\n",
        "save_predictions(train_predictions, 'bert_predictions_train.csv')\n",
        "save_predictions(val_predictions, 'bert_predictions_val.csv')\n",
        "save_predictions(test_history_predictions, 'bert_predictions_history.csv')\n",
        "save_predictions(test_future_predictions, 'bert_predictions_future.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save performance metrics\n",
        "print(\"\\nSaving performance metrics...\")\n",
        "with open('/kaggle/working/bert_performance.json', 'w') as f:\n",
        "    json.dump(performance_metrics, f, indent=2)\n",
        "print(\"✓ Saved bert_performance.json\")\n",
        "\n",
        "# Save training log\n",
        "training_log = {\n",
        "    'training_loss': float(train_result.training_loss),\n",
        "    'log_history': trainer.state.log_history\n",
        "}\n",
        "with open('/kaggle/working/bert_training_log.json', 'w') as f:\n",
        "    json.dump(training_log, f, indent=2, default=str)\n",
        "print(\"✓ Saved bert_training_log.json\")\n",
        "\n",
        "# Save aggregated statistics\n",
        "with open('/kaggle/working/bert_aggregated_stats.json', 'w') as f:\n",
        "    json.dump(performance_metrics, f, indent=2)\n",
        "print(\"✓ Saved bert_aggregated_stats.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "All outputs have been saved to `/kaggle/working/`:\n",
        "\n",
        "**Predictions:**\n",
        "- `bert_predictions_train.csv`\n",
        "- `bert_predictions_val.csv`\n",
        "- `bert_predictions_history.csv`\n",
        "- `bert_predictions_future.csv`\n",
        "\n",
        "**Metrics & Statistics:**\n",
        "- `bert_performance.json` - Detailed metrics (RMSE, MAE, R²) for all splits\n",
        "- `bert_training_log.json` - Training history\n",
        "- `bert_aggregated_stats.json` - Aggregated statistics for local comparison\n",
        "\n",
        "**Diagnostic Plots (10 total):**\n",
        "- All plots saved to `/kaggle/working/figures/`\n",
        "\n",
        "You can download all files from the \"Output\" tab or via the file browser on the right.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
